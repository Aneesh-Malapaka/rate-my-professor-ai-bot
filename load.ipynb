{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os #to access the environment variables\n",
    "import pinecone\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
    "from llama_index.embeddings.gemini import GeminiEmbedding\n",
    "from llama_index.core import StorageContext, VectorStoreIndex, download_loader\n",
    "import google.generativeai as genai\n",
    "from llama_index.core import Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating db instance\n",
    "pc = Pinecone(api_key=os.getenv('PINECONE_API_KEY'))\n",
    "\n",
    "#creating index on pinecone (comment out when using locally)\n",
    "# pc.create_index(\n",
    "#     name=\"rag-rmp\",\n",
    "#     dimension=768,\n",
    "#     metric=\"cosine\",\n",
    "#     spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the reviews file\n",
    "import json\n",
    "data = json.load(open(\"reviews.json\"))\n",
    "# data['reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#By default, LlamaIndex assumes you're using OpenAI to generate embeddings. To configure it to use Gemini instead, we need to set up the service context which lets LlamaIndex know which llm and which embedding model to use.\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv('GEMINI_API_KEY')\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "client = Gemini()\n",
    "# embed_model = GeminiEmbedding(\n",
    "#     model_name=\"models/embedding-004\"\n",
    "# )\n",
    "\n",
    "# Settings.embed_model = embed_model\n",
    "# Settings.llm = client\n",
    "# Settings.chunk_size = 512\n",
    "\n",
    "processed_data = []\n",
    "for review in data[\"reviews\"]:\n",
    "    response = genai.embed_content(\n",
    "        model=\"models/text-embedding-004\",\n",
    "        content=review[\"review\"],\n",
    "        task_type=\"retrieval_document\",\n",
    "        title='review of '+review[\"professor_name\"] \n",
    "    )\n",
    "   \n",
    "    embedding = response[\"embedding\"]\n",
    "    processed_data.append({\n",
    "        \"values\":embedding,\n",
    "        \"id\": review[\"professor_name\"],\n",
    "        \"metadata\":{\n",
    "            \"subject\": review[\"subject\"],\n",
    "            \"domain\": review[\"domain\"],\n",
    "            \"rating\": review[\"rating\"],\n",
    "            \"review\": review[\"review\"],\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'upserted_count': 40}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating index and upsert (insert into db) upsert operation writes vectors into a namespace. If a new value is upserted for an existing vector id, it will overwrite the previous value.\n",
    "index = pc.Index('rag-rmp') #collection\n",
    "index.upsert(\n",
    "    vectors=processed_data,\n",
    "    namespace=\"namespace1\" #document\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 768,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'namespace1': {'vector_count': 40}},\n",
       " 'total_vector_count': 40}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#index description\n",
    "index.describe_index_stats()\n",
    "#{'dimension': 768,\n",
    "#'index_fullness': 0.0,\n",
    "# 'namespaces': {'namespace1': {'vector_count': 40}},\n",
    "# 'total_vector_count': 40}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
